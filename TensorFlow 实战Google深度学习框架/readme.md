# TensorFlow  实战Google深度学习框架 阅读笔记
## 前言

最近看了同事的这本书， 在网上也断断续续看了不是机器学的知识。发现看书获得的知识更系统，也明白了许多以前不太明白的地方。在此记下获得新的知识点。    

>
	教育就是当一个人把在学校所学全部忘光之后剩下的东西。 
	Education is what remains after one has forgotten everything one has learned in school.
                                                                             ——爱因斯坦

## 本书例子代码 
[github 地址](https://github.com/caicloud/tensorflow-tutorial)

## 神经元 
* 神经元的输入是特征向量（feature）x 
* 最简单的神经元结构的输出就是所有输入的加权和   是线性的
* 对每一项实施激活函数f(x)  就可以实现非线性的神经元

## 前向传播
* 前向传播就是计算每个神经元值的过程（已知权重w，层数，和每层神经元的个数 即已知网络的结构）
* 计算过程是最基本的是加权求和的过程，可以用矩阵乘法表达。
* 矩阵乘法在tensorflow中用tf.matmul()实现。 很简单。 嗯，tensorflow就是把这些理论封装成了代码。

## 反向传播

## tensorflow 训练的过程
1.初始化 -》 2.选取一部分训练 -》 3.通过前向传播获得预测值 -》4.通过反向传播更新变量 

## 深度学习与深层神经网络

###### 深度学习定义（维基百科）：一类通过多层非线性变换对高复杂性数据建模算法的合集

因为深层神经网络是实现 “多层非线性变换” 最常用的一种方法，所以在实际中基本上可以认为深度学习就是深层神经网络的代名词。

### 激活函数
* 激活函数实现去线性化  生活中很多模型是非线性的

### 多层网络 解决 异或 运算
* 感知机（单层网络）无法解决异或运算
* 加入隐藏层可以很好的解决
* 深层神经网络有自动提取特征的功能。
* 自动提取特征对解决不易提取特征向量的问题（比如图片识别、语音识别等）有很大帮助。 
* 特征提取很难的。特征我也理解为属性。 比如如何定义“人”，我们在不同的场景下提取的特征（属性）可能不一样。并且有些东西很难提取特征。

## 损失函数
判断模型输出向量和真实向量有多接近

### 交叉熵
* 交叉熵刻画了两个概率分布之间的距离，它是分类问题中使用比较广的一种损失函数
* 交叉熵是一个信息论中的概念，它原本是用来估算平均编码长度的。机器学习借用了。数学的通用的

### softmax
* 在 tensorflow 中用softmax将神经网络的输出变成一个概率分布。之后就可以用交叉熵得到损失函数了。

### tensorflow 可以自定义损失函数
* todo：




## 神经网络优化算法即如何训练如何调整参数W和偏置b的方法




```flow
st=>start: Start
op=>operation: Your Operation
cond=>condition: Yes or No?
e=>end
st->op->cond
cond(yes)->e
cond(no)->op
```



