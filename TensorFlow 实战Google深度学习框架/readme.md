# TensorFlow  实战Google深度学习框架 阅读笔记
## 前言

最近看了同事的这本书， 在网上也断断续续看了不是机器学的知识。发现看书获得的知识更系统，也明白了许多以前不太明白的地方。在此记下获得新的知识点。    

>
	教育就是当一个人把在学校所学全部忘光之后剩下的东西。 
	Education is what remains after one has forgotten everything one has learned in school.
                                                                             ——爱因斯坦

## 本书例子代码 
[github 地址](https://github.com/caicloud/tensorflow-tutorial)

## 神经元 
* 神经元的输入是特征向量（feature）x 
* 最简单的神经元结构的输出就是所有输入的加权和   是线性的
* 对每一项实施激活函数f(x)  就可以实现非线性的神经元

## 前向传播
* 前向传播就是计算每个神经元值的过程（已知权重w，层数，和每层神经元的个数 即已知网络的结构）
* 计算过程是最基本的是加权求和的过程，可以用矩阵乘法表达。
* 矩阵乘法在tensorflow中用tf.matmul()实现。 很简单。 嗯，tensorflow就是把这些理论封装成了代码。

## 反向传播

## tensorflow 训练的过程
1.初始化 -》 2.选取一部分训练 -》 3.通过前向传播获得预测值 -》4.通过反向传播更新变量 

## 深度学习与深层神经网络

###### 深度学习定义（维基百科）：一类通过多层非线性变换对高复杂性数据建模算法的合集

因为深层神经网络是实现 “多层非线性变换” 最常用的一种方法，所以在实际中基本上可以认为深度学习就是深层神经网络的代名词。

### 激活函数
* 激活函数实现去线性化  生活中很多模型是非线性的

### 多层网络 解决 异或 运算
* 感知机（单层网络）无法解决异或运算
* 加入隐藏层可以很好的解决
* 深层神经网络有自动提取特征的功能。
* 自动提取特征对解决不易提取特征向量的问题（比如图片识别、语音识别等）有很大帮助。 
* 特征提取很难的。特征我也理解为属性。 比如如何定义“人”，我们在不同的场景下提取的特征（属性）可能不一样。并且有些东西很难提取特征。

## 损失函数
判断模型输出向量和真实向量有多接近

### 交叉熵
* 交叉熵刻画了两个概率分布之间的距离，它是分类问题中使用比较广的一种损失函数
* 交叉熵是一个信息论中的概念，它原本是用来估算平均编码长度的。机器学习借用了。数学的通用的

### softmax
* 在 tensorflow 中用softmax将神经网络的输出变成一个概率分布。之后就可以用交叉熵得到损失函数了。

### 与分类问题不同，回归问题解决的事对具体数值的预测。比如房价的预测。解决回归问题的神经网络一般只有一个输出节点，这个节点的输出技术预测值（房价具体事多少）。对于回归问题，最常用的损失函数是 **均方误差**。 

### tensorflow 可以自定义损失函数
* todo：




## 神经网络优化算法
* 如何训练如何调整参数W和偏置b的方法   
* 通过反向传播算法和梯度下降算法调整神经网络中参数等取值
* 求函数的最小值。J(𝝷)最小值。通过求导（梯度） 迭代（反向传播调整参数
* 迭代中需要一个学习效率𝝶。每次移动参数的幅度
* 梯度下降得到的是局部最优解
* **随机梯度下降** 减少训练时间
* 实际应用中采用折中方法，每次计算一小部分数据的损失。这一小部分数据呗称之为 ** batch **
##### 以上在tensorflow开发中，使用 batch 就到达了优化，具体的实现tf内部实现了。

## 神经网络进一步优化
### 学习率的设置
* 学习率决定了参数每次更新的幅度
* tensorflow 一般使用“指数衰减法”tf.train.exponential_decay()
### 过拟合化
* 为了避免过拟合，常用的方法是正则化。正则化的思想是在损失函数中加入刻画模型复杂度的指标。一般模型复杂度只由权重W决定。L1，L2都限制权重的大小。
	* L1 正则化 w  让参数变得稀疏  就说很多参数变为了0
	* L2 正则化 W^2
	* 对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）
	* L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
    * L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合
    * 这里讲的比较清楚 [机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)
* Dropout
	* L1、L2正则化是通过改动代价函数来实现的，而Dropout则是通过改动神经网络本身来实现的，它是在训练网络时用的一种技巧（trike）。
* 数据集扩增（data augmentation）能够在原始数据上做些改动，得到很多其它的数据，以图片数据集举例，能够做各种变换，如：      
	* 将原始图片旋转一个小角度
	* 加入随机噪声
	* 一些有弹性的畸变（elastic distortions）。论文《Best practices for convolutional neural networks applied to visual document analysis》对MNIST做了各种变种扩增。
	* 截取（crop）原始图片的一部分。

### 滑动平均模型
* 想想滑动窗口 想想平均值 期望的含义。
* [参考这里](https://www.cnblogs.com/cloud-ken/p/7521609.html)


https://blog.csdn.net/jiaoyangwm/article/category/7439149


```flow
st=>start: Start
op=>operation: Your Operation
cond=>condition: Yes or No?
e=>end
st->op->cond
cond(yes)->e
cond(no)->op
```

<!-- <meta http-equiv="refresh" content="1"> -->

